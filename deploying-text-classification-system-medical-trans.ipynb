{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":127612,"sourceType":"datasetVersion","datasetId":64826}],"dockerImageVersionId":30235,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Contents**\n\n#### [1.0 Import Functions and EDA](#1.0)\n* [1.1 Import Functions](#1.1)\n* [1.2 Word Counts of Each Medical Specialty](#1.2)\n* [1.3 Sample Size of Each Medical Specialty](#1.3)\n* [1.4 General Cleaning](#1.4)\n\n#### [2.0 Text Normalisation](#2.0)\n* [2.1 Lower Case](#2.1)\n* [2.2 Remove Punctuation and Numbers](#2.2)\n* [2.3 Tokenisation](#2.3)\n* [2.4 Stemming ](#2.4)\n\n#### [3.0 Text N-Gram Feature Extraction](#3.0)\n* [3.1 Extract 5 Types of N-Gram](#3.1)\n* [3.2 Dimension of Each Feature Vector](#3.2)\n\n#### [4.0 Text Classification Modelling](#4.0)\n* [4.1 Visualising Classification Prediction](#4.1)\n* [4.2 Dimensionality Reduction](#4.2)\n* [4.3 Obtain Best Classifier and Feature Vector](#4.3)\n* [4.4 Evaluate on Each Class Labels](#4.4)","metadata":{}},{"cell_type":"markdown","source":"# IMPORT FUNCTIONS AND  EDA","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nimport warnings\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom nltk.tokenize import WhitespaceTokenizer\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nfrom sklearn import preprocessing\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport os","metadata":{"execution":{"iopub.status.busy":"2025-04-02T08:45:36.934285Z","iopub.execute_input":"2025-04-02T08:45:36.934683Z","iopub.status.idle":"2025-04-02T08:45:37.591623Z","shell.execute_reply.started":"2025-04-02T08:45:36.934652Z","shell.execute_reply":"2025-04-02T08:45:37.590533Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def trim(df):\n    df.columns = df.columns.str.strip()\n    df = df.drop_duplicates()\n    df.columns = df.columns.str.lower()\n    df.columns = df.columns.str.replace(' ','_')\n    df_obj = df.select_dtypes(['object'])\n    df[df_obj.columns] = df_obj.apply(lambda x: x.str.strip())\n    print(\"All column names have been striped, lowered case, replaced space with underscore if any\")\n    print(\"Dropped duplicated instances if any\")\n    print(\"Categorical instances have been striped\")\n    return df\n\npd.set_option('display.max_colwidth', 255)\ndf =pd.read_csv('/kaggle/input/medicaltranscriptions/mtsamples.csv')\ndf.drop('Unnamed: 0', axis=1, inplace=True)\ndf = trim(df)\n\ndef vc(df, column, r=False):\n    vc_df = df.reset_index().groupby([column]).size().to_frame('count')\n    vc_df['percentage (%)'] = vc_df['count'].div(sum(vc_df['count'])).mul(100)\n    vc_df = vc_df.sort_values(by=['percentage (%)'], ascending=False)\n    if r:\n        return vc_df\n    else:\n        print(f'STATUS: Value counts of \"{column}\"...')\n        display(vc_df)\n        \ndef shape(df,df_name):\n    print(f'STATUS: Dimension of \"{df_name}\" = {df.shape}')\n        \ndf.head(3)\nshape(df,'df')","metadata":{"execution":{"iopub.status.busy":"2025-04-02T08:46:47.889248Z","iopub.execute_input":"2025-04-02T08:46:47.889655Z","iopub.status.idle":"2025-04-02T08:46:48.110080Z","shell.execute_reply.started":"2025-04-02T08:46:47.889620Z","shell.execute_reply":"2025-04-02T08:46:48.108830Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df[df['medical_specialty'].isin(['Neurosurgery','ENT - Otolaryngology','Discharge Summary'])]\nshape(df,'df')","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:25:42.229291Z","iopub.execute_input":"2025-03-31T07:25:42.229561Z","iopub.status.idle":"2025-03-31T07:25:42.236763Z","shell.execute_reply.started":"2025-03-31T07:25:42.229537Z","shell.execute_reply":"2025-03-31T07:25:42.235737Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"classes","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load the dataset (update file path if needed)\ndf = pd.read_csv(\"/kaggle/input/medicaltranscriptions/mtsamples.csv\")  # Replace with actual dataset file\n\n# Check unique classes in 'medical_speciality' column\nunique_classes = df[\"medical_specialty\"].unique()\nnum_classes = len(unique_classes)\n\nprint(f\"Number of unique medical specialties: {num_classes}\")\nprint(\"List of medical specialties:\")\nprint(unique_classes)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T08:37:55.863455Z","iopub.execute_input":"2025-04-02T08:37:55.864308Z","iopub.status.idle":"2025-04-02T08:37:56.061848Z","shell.execute_reply.started":"2025-04-02T08:37:55.864271Z","shell.execute_reply":"2025-04-02T08:37:56.060623Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the dataset (update file path if needed)\ndf = pd.read_csv(\"/kaggle/input/medicaltranscriptions/mtsamples.csv\")  # Replace with actual dataset file\n\n# Count the occurrences of each class\nclass_counts = df[\"medical_specialty\"].value_counts()\n\n# Plot the distribution\nplt.figure(figsize=(12, 6))\nsns.barplot(x=class_counts.index, y=class_counts.values, palette=\"viridis\")\nplt.xticks(rotation=90)  # Rotate labels for better visibility\nplt.xlabel(\"Medical Specialties\")\nplt.ylabel(\"Count\")\nplt.title(\"Distribution of Medical Specialties in Dataset\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T08:39:03.841804Z","iopub.execute_input":"2025-04-02T08:39:03.842448Z","iopub.status.idle":"2025-04-02T08:39:04.488123Z","shell.execute_reply.started":"2025-04-02T08:39:03.842407Z","shell.execute_reply":"2025-04-02T08:39:04.486943Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# WORD COUNTS OF EACH MEDICAL SPECIALTY","metadata":{}},{"cell_type":"markdown","source":"To query the data, I would like to know how is the size of the dataset and also to rank null values in descending order","metadata":{}},{"cell_type":"code","source":"medical_specialty_list = [] ; word_count_list =[]\nfor medical_specialty in df['medical_specialty'].unique():\n    df_filter = df.loc[(df['medical_specialty'] == medical_specialty)]\n    word_count_temp = df_filter['transcription'].str.split().str.len().sum()\n    medical_specialty_list.append(medical_specialty)\n    word_count_list.append(word_count_temp)\nword_count_df = pd.DataFrame({'Medical Specialty':medical_specialty_list, 'Word Count':word_count_list})\nword_count_df['Word Count'] = word_count_df['Word Count'].astype('int')\nword_count_df = word_count_df.sort_values('Word Count', ascending=False)\nword_count_df.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:25:42.238172Z","iopub.execute_input":"2025-03-31T07:25:42.238453Z","iopub.status.idle":"2025-03-31T07:25:42.283830Z","shell.execute_reply.started":"2025-03-31T07:25:42.238426Z","shell.execute_reply":"2025-03-31T07:25:42.282320Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"total_word_count = df['transcription'].str.split().str.len().sum()\nprint(f'The word count of all transcription is: {int(total_word_count)}')","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:25:42.285491Z","iopub.execute_input":"2025-03-31T07:25:42.285901Z","iopub.status.idle":"2025-03-31T07:25:42.315257Z","shell.execute_reply.started":"2025-03-31T07:25:42.285861Z","shell.execute_reply":"2025-03-31T07:25:42.313875Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# SAMPLE SIZE OF EACH MEDICAL SPECIALTY","metadata":{}},{"cell_type":"code","source":"vc(df, 'medical_specialty')","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:25:42.316779Z","iopub.execute_input":"2025-03-31T07:25:42.317983Z","iopub.status.idle":"2025-03-31T07:25:42.343885Z","shell.execute_reply.started":"2025-03-31T07:25:42.317913Z","shell.execute_reply":"2025-03-31T07:25:42.342649Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# GENERAL CLEANING","metadata":{}},{"cell_type":"code","source":"# to print data shape\nprint(f'data shape is: {df.shape}')\n\n# to identify the null values by descending order\ndf.isnull().sum().sort_values(ascending = False)","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:25:42.356157Z","iopub.execute_input":"2025-03-31T07:25:42.356599Z","iopub.status.idle":"2025-03-31T07:25:42.366666Z","shell.execute_reply.started":"2025-03-31T07:25:42.356570Z","shell.execute_reply":"2025-03-31T07:25:42.365516Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"One important detail is that I found out there are 2 rows containing no transcription. They should be removed as transcription is our only predictors in this text classification task.","metadata":{}},{"cell_type":"code","source":"# to remove transcription rows that is empty\ndf = df[df['transcription'].notna()]\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:25:42.368387Z","iopub.execute_input":"2025-03-31T07:25:42.369075Z","iopub.status.idle":"2025-03-31T07:25:42.391496Z","shell.execute_reply.started":"2025-03-31T07:25:42.369043Z","shell.execute_reply":"2025-03-31T07:25:42.390377Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"After dropping the null values, there are no null values for the transcription attribute. ","metadata":{}},{"cell_type":"code","source":"# drop redundant columns\ndf =df.drop(['description','sample_name','keywords'], axis=1)\ndf.head(2)","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:25:42.393033Z","iopub.execute_input":"2025-03-31T07:25:42.393448Z","iopub.status.idle":"2025-03-31T07:25:42.409614Z","shell.execute_reply.started":"2025-03-31T07:25:42.393408Z","shell.execute_reply":"2025-03-31T07:25:42.408495Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The target labels (or the topic) is the 'medical_specialty' attribute. Now, let's identify how is the value counts of the target labels, and as well visualise it in a bar chart. In order to visualise in matplotlib, function of flattening list is defined in order to put the target value counts into the matplotlib function.","metadata":{}},{"cell_type":"markdown","source":"The target labels is quite balanced","metadata":{"execution":{"iopub.status.busy":"2022-09-14T13:38:45.540435Z","iopub.execute_input":"2022-09-14T13:38:45.540923Z","iopub.status.idle":"2022-09-14T13:38:45.548809Z","shell.execute_reply.started":"2022-09-14T13:38:45.540883Z","shell.execute_reply":"2022-09-14T13:38:45.546804Z"}}},{"cell_type":"markdown","source":"# TEXT NORMALIZATION","metadata":{}},{"cell_type":"markdown","source":"Data normalisation will be conducted for the trascription. One of the reasons is to convert the transcript into standard format, which important for data extraction later. In this data normalisation task, following task will be executed, which are:\n1. Lowe Case\n2. Removing punctuation and numbers\n3. Tokenisation of the transcription\n4. Lemmatisation\n5. Remove Stop Words","metadata":{"execution":{"iopub.status.busy":"2022-09-14T13:39:00.032363Z","iopub.execute_input":"2022-09-14T13:39:00.032844Z","iopub.status.idle":"2022-09-14T13:39:00.043819Z","shell.execute_reply.started":"2022-09-14T13:39:00.032806Z","shell.execute_reply":"2022-09-14T13:39:00.041823Z"}}},{"cell_type":"markdown","source":"# LOWER CASE","metadata":{}},{"cell_type":"code","source":"# To convert transcription into lowercase\ndef lower(df, attribute):\n    df.loc[:,attribute] = df[attribute].apply(lambda x : str.lower(x))\n    return df\ndf = lower(df,'transcription')\ndf.head(3)","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:25:42.411129Z","iopub.execute_input":"2025-03-31T07:25:42.411524Z","iopub.status.idle":"2025-03-31T07:25:42.431047Z","shell.execute_reply.started":"2025-03-31T07:25:42.411485Z","shell.execute_reply":"2025-03-31T07:25:42.429516Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# REMOVE PUNCTUATIONS AND NUMBERS","metadata":{}},{"cell_type":"code","source":"# To remove transcription punctuation and numbers\n\nwarnings.filterwarnings('ignore')\ndef remove_punc_num(df, attribute):\n    df.loc[:,attribute] = df[attribute].apply(lambda x : \" \".join(re.findall('[\\w]+',x)))\n    df[attribute] = df[attribute].str.replace('\\d+', '')\n    return df\ndf =remove_punc_num(df, 'transcription')\ndf_no_punc =df.copy()\ndf.head(3)","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:25:42.432827Z","iopub.execute_input":"2025-03-31T07:25:42.433231Z","iopub.status.idle":"2025-03-31T07:25:42.516717Z","shell.execute_reply.started":"2025-03-31T07:25:42.433201Z","shell.execute_reply":"2025-03-31T07:25:42.515584Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TOKENIZATION\n","metadata":{}},{"cell_type":"code","source":"# to tokenise transcription\n\n# import nltk\ntk =WhitespaceTokenizer()\ndef tokenise(df, attribute):\n    df['tokenised'] = df.apply(lambda row: tk.tokenize(str(row[attribute])), axis=1)\n    return df\ndf =tokenise(df, 'transcription')\ndf_experiment =df.copy()\ndf.head(3)","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:25:42.518041Z","iopub.execute_input":"2025-03-31T07:25:42.518715Z","iopub.status.idle":"2025-03-31T07:25:42.599071Z","shell.execute_reply.started":"2025-03-31T07:25:42.518683Z","shell.execute_reply":"2025-03-31T07:25:42.597970Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# STEMMING","metadata":{}},{"cell_type":"code","source":"from nltk.stem.snowball import SnowballStemmer\ndef stemming(df, attribute):\n    # Use English stemmer.\n    stemmer = SnowballStemmer(\"english\")\n    df['stemmed'] = df[attribute].apply(lambda x: [stemmer.stem(y) for y in x]) # Stem every word.\n    return df\ndf =stemming(df_experiment, 'tokenised')\ndf.head(2)","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:25:42.600301Z","iopub.execute_input":"2025-03-31T07:25:42.600603Z","iopub.status.idle":"2025-03-31T07:25:44.252720Z","shell.execute_reply.started":"2025-03-31T07:25:42.600576Z","shell.execute_reply":"2025-03-31T07:25:44.251720Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# STOP WORDS REMOVAL\n\n\nRemoving stop words from the feature space, otherwise it will affect the classifier performance as the collection frequency is often high","metadata":{}},{"cell_type":"code","source":"# Showing the list of the English stop words, it has a number of 179 stop words in this list\n\nstop = stopwords.words('english')\nprint(f\"There are {len(stop)} stop words \\n\")\nprint(stop)","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:25:44.254334Z","iopub.execute_input":"2025-03-31T07:25:44.254725Z","iopub.status.idle":"2025-03-31T07:25:44.266457Z","shell.execute_reply.started":"2025-03-31T07:25:44.254686Z","shell.execute_reply":"2025-03-31T07:25:44.265420Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Removing stop words\ndef remove_stop_words(df, attribute):\n    stop = stopwords.words('english')\n    df['stemmed_without_stop'] = df[attribute].apply(lambda x: ' '.join([word for word in x if word not in (stop)]))\n    return df\ndf = remove_stop_words(df, 'stemmed')\ndf.head(2)","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:25:44.267991Z","iopub.execute_input":"2025-03-31T07:25:44.268399Z","iopub.status.idle":"2025-03-31T07:25:44.553361Z","shell.execute_reply.started":"2025-03-31T07:25:44.268359Z","shell.execute_reply":"2025-03-31T07:25:44.552019Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"After the 5 data normalisation steps, each transcription record is now in a standard format, which is ready for the n-gram features extraction later. Hence, we should use the attribute 'stemmed_withou_stop' as the predictor attribute and drop other redundant attributes, namely 'transcription', 'tokenized_transcription' and 'stemmed'.","metadata":{}},{"cell_type":"code","source":"df =df.drop(['transcription','stemmed', 'tokenised'], axis=1)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:25:44.554742Z","iopub.execute_input":"2025-03-31T07:25:44.555143Z","iopub.status.idle":"2025-03-31T07:25:44.567297Z","shell.execute_reply.started":"2025-03-31T07:25:44.555113Z","shell.execute_reply":"2025-03-31T07:25:44.566203Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"total_word_count_normalised = df['stemmed_without_stop'].str.split().str.len().sum()\nprint(f'The word count of transcription after normalised is: {int(total_word_count_normalised)}')\nprint(f'{round((total_word_count - total_word_count_normalised)/total_word_count*100, 2)}% less word')","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:25:44.568661Z","iopub.execute_input":"2025-03-31T07:25:44.569083Z","iopub.status.idle":"2025-03-31T07:25:44.593558Z","shell.execute_reply.started":"2025-03-31T07:25:44.569042Z","shell.execute_reply":"2025-03-31T07:25:44.592400Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"le = preprocessing.LabelEncoder()\nle.fit(df['medical_specialty'])\ndf['encoded_target'] = le.transform(df['medical_specialty'])\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:25:44.594990Z","iopub.execute_input":"2025-03-31T07:25:44.595328Z","iopub.status.idle":"2025-03-31T07:25:44.613879Z","shell.execute_reply.started":"2025-03-31T07:25:44.595299Z","shell.execute_reply":"2025-03-31T07:25:44.612817Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# TEXT N-GRAM FEATURE EXTRACTION","metadata":{}},{"cell_type":"markdown","source":"We will use sklearn class 'CountVectoriser' to extract different n-grams features. In order to do so, the transcription should be converted into a list format, rather than a dataframe. For the purpose of converting into a flat list (i.e., there is no inner list), the function of 'flat_list' that defined above is used.","metadata":{}},{"cell_type":"code","source":"# function to flatten one list\ndef flat_list(unflat_list):\n    flatted = [item for sublist in unflat_list for item in sublist]\n    return flatted\n\ndef to_list(df, attribute):\n    # Select the normalised transcript column \n    df_transcription = df[[attribute]]\n    # To convert the attribute into list format, but it has inner list. So it cannot put into the CountVectoriser\n    unflat_list_transcription = df_transcription.values.tolist()\n    # Let's use back the function defined above, \"flat_list\", to flatten the list\n    flat_list_transcription = flat_list(unflat_list_transcription)\n    return flat_list_transcription\nflat_list_transcription = to_list(df, 'stemmed_without_stop')","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:25:44.615212Z","iopub.execute_input":"2025-03-31T07:25:44.615514Z","iopub.status.idle":"2025-03-31T07:25:44.631474Z","shell.execute_reply.started":"2025-03-31T07:25:44.615486Z","shell.execute_reply":"2025-03-31T07:25:44.630253Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EXTRACT 5 TYPES OF N-GRAM","metadata":{}},{"cell_type":"markdown","source":"CountVectorizer is used to convert a collection of transcript documents to a matrix of n-gram features. To explain the ngram_range, all values of n such such that min_n <= n <= max_n will be used. For example an ngram_range of (1, 1) means only unigrams, (1, 2) means unigrams and bigrams, and (2, 2) means only bigrams.","metadata":{}},{"cell_type":"code","source":"n_gram_features ={'unigram':(1,1),'unigram_bigram':(1,2),'bigram':(2,2),\\\n       'bigram_trigram':(2,3),'trigram':(3,3)}\nfeature_name=[]\ntemp=[]\nfor key, values in n_gram_features.items():\n    temp.append(key)\n    feature_name.append(key)\ntemp","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:25:44.632849Z","iopub.execute_input":"2025-03-31T07:25:44.633689Z","iopub.status.idle":"2025-03-31T07:25:44.644215Z","shell.execute_reply.started":"2025-03-31T07:25:44.633659Z","shell.execute_reply":"2025-03-31T07:25:44.643186Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_n_gram_features(flat_list_transcription):\n    temp=[]\n    for key, values in n_gram_features.items(): \n        vectorizer = CountVectorizer(ngram_range=values)\n        vectorizer.fit(flat_list_transcription)\n        temp.append(vectorizer.transform(flat_list_transcription))\n    return temp\ntemp = generate_n_gram_features(flat_list_transcription)","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:25:44.645490Z","iopub.execute_input":"2025-03-31T07:25:44.645785Z","iopub.status.idle":"2025-03-31T07:25:46.530442Z","shell.execute_reply.started":"2025-03-31T07:25:44.645759Z","shell.execute_reply":"2025-03-31T07:25:46.529211Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DIMENSION OF EACH FEATIRE VECTOR","metadata":{}},{"cell_type":"code","source":"dataframes = {'unigram':temp[0], \n              'unigram_bigram':temp[1], \n              'bigram':temp[2], \n              'bigram_trigram':temp[3], \n              'trigram':temp[4]}\nfeature_vector = [] ; feature_vector_shape = []\nfor key in dataframes:\n    feature_vector.append(key)\n    feature_vector_shape.append(dataframes[key].shape)\n\nn_gram_df = pd.DataFrame({'N-Gram Feature Vector':feature_vector, 'Data Dimension':feature_vector_shape})\nn_gram_df","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:25:46.535315Z","iopub.execute_input":"2025-03-31T07:25:46.535662Z","iopub.status.idle":"2025-03-31T07:25:46.550024Z","shell.execute_reply.started":"2025-03-31T07:25:46.535629Z","shell.execute_reply":"2025-03-31T07:25:46.548875Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"After the feature extraction process, 5 kinds of n-gram features are extracted. It is interesting to notice that when the number of 'n' getting higher (i.e, n=1:unigram, n=2:bigram, n=3:trigram), there is a higer number of columns. This is due to it is getting harder to find similar features that can be stored in similar column when it has a longer connected words as one featuer. If the feature is unique, it will automatically append additional column to store the feaure.","metadata":{}},{"cell_type":"code","source":"# to retrieve a unigram feature vector\ndataframes['unigram']","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:25:46.551369Z","iopub.execute_input":"2025-03-31T07:25:46.551673Z","iopub.status.idle":"2025-03-31T07:25:46.565274Z","shell.execute_reply.started":"2025-03-31T07:25:46.551647Z","shell.execute_reply":"2025-03-31T07:25:46.564222Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#TEXT CLASSIFICATION MODELLING","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nimport warnings\nfrom sklearn.experimental import enable_halving_search_cv  # noqa\nfrom sklearn.model_selection import HalvingGridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import classification_report\n\nwarnings.filterwarnings('ignore')\nrandom_state_number =8888\ndf_target =df[['encoded_target']].values.ravel()","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:25:46.566917Z","iopub.execute_input":"2025-03-31T07:25:46.567333Z","iopub.status.idle":"2025-03-31T07:25:46.713378Z","shell.execute_reply.started":"2025-03-31T07:25:46.567299Z","shell.execute_reply":"2025-03-31T07:25:46.712341Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"metrics = {\n    'f1':[f1_score, 'f1_macro'], \n    'precision': [precision_score, 'precision_macro'], \n    'recall': [recall_score, 'recall_macro']\n}\n\n# get evaluation result\n\ndef get_performance(param_grid, base_estimator, dataframes):\n    df_name_list =[]; best_estimator_list=[]; best_score_list=[]; test_predict_result_list=[];\n    metric_list = [];\n    \n    for df_name, df in dataframes.items():\n        \n        X_train, X_test, y_train, y_test = train_test_split(df, df_target, test_size=0.2, random_state=random_state_number)\n        for _, metric_dict in metrics.items():\n            sh = HalvingGridSearchCV(base_estimator, param_grid, cv=5, scoring=metric_dict[1],random_state=random_state_number,\n                                      factor=2).fit(X_train, y_train)\n\n            best_estimator = sh.best_estimator_\n            clf = best_estimator.fit(X_train, y_train)\n            prediction = clf.predict(X_test)\n            test_predict_result = metric_dict[0](y_test, prediction, average='macro')\n\n            df_name_list.append(df_name) ; best_estimator_list.append(best_estimator) ; \n            best_score_list.append(sh.best_score_) ; \n            test_predict_result_list.append(test_predict_result) ;metric_list.append(metric_dict[1])\n            \n            \n    model_result = pd.DataFrame({'Vector':df_name_list,'Metric':metric_list,\n                               'Calibrated Estimator':best_estimator_list,\n                               'Best CV Metric Score':best_score_list, 'Test Predict Metric Score': test_predict_result_list})\n    return model_result\n","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:25:46.714990Z","iopub.execute_input":"2025-03-31T07:25:46.715703Z","iopub.status.idle":"2025-03-31T07:25:46.726560Z","shell.execute_reply.started":"2025-03-31T07:25:46.715670Z","shell.execute_reply":"2025-03-31T07:25:46.725385Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# VISUALIZING CLASSIFICATION PREDICTION","metadata":{}},{"cell_type":"code","source":"font = {'family' : 'Tahoma',\n        'weight' : 'bold',\n        'size'   : 12}\nmatplotlib.rc('font', **font)\n\ndef vis_classification(vector_type = 'unigram', estimator = KNeighborsClassifier(n_neighbors=9)):\n    pca = PCA(n_components=2)\n    df1 = pca.fit_transform(dataframes[vector_type].todense())\n    X_train, X_test, y_train, y_test = train_test_split(df1, df_target, test_size=0.2, random_state=random_state_number)\n    \n    # get training set\n    df2 = pd.DataFrame({'pca1':X_train[:,1], 'pca2': X_train[:,0], 'y':le.inverse_transform(y_train)})\n    min_1, max_1 = df2['pca1'].min(), df2['pca1'].max()\n    min_2, max_2 = df2['pca2'].min(), df2['pca2'].max()\n    \n    # generate dimension reduced, but extended data\n    pca1_range = np.linspace(min_1,max_1,30)\n    pca2_range = np.linspace(min_2,max_2,30)\n    \n    # shuffle\n    np.random.shuffle(pca1_range) ; np.random.shuffle(pca2_range)\n    \n    # to dataframe\n    prediction_test = pd.DataFrame({'pca1':pca1_range, 'pca2':pca2_range})\n\n    best_estimator = estimator\n    \n    # fit training set and predict extended data\n    clf = best_estimator.fit(X_train, y_train)\n\n    fig, axs = plt.subplots(nrows = 1, ncols = 2, figsize=(15,6))\n    cmap = plt.cm.get_cmap('tab10', 4)\n    fig.suptitle(f\"Visualising {type(estimator).__name__} on {vector_type.capitalize()} Vector\", fontsize=14,fontweight='bold')\n\n\n    def plot_scatter(ax, predictor_set, target, title):\n        \n        # plot area classifier\n        clf = best_estimator.fit(X_train, y_train)\n        axs[0].tricontourf(X_train[:,0], X_train[:,1], clf.predict(X_train), levels=np.arange(-0.5, 4), zorder=10, alpha=0.3, cmap=cmap, edgecolors=\"k\")\n        \n        axs[1].tricontourf(X_test[:,0], X_test[:,1], clf.predict(X_test), levels=np.arange(-0.5, 4), zorder=10, alpha=0.3, cmap=cmap, edgecolors=\"k\")\n        \n        # plot scatter\n        df3 = pd.DataFrame({'pca1':predictor_set[:,1], 'pca2': predictor_set[:,0], 'y':le.inverse_transform(target)})\n        for y_label in df3['y'].unique():\n            df_filter = df3[df3['y']==y_label]\n            ax.scatter(df_filter['pca1'], df_filter['pca2'], alpha=1,label=f\"{y_label}\")\n        ax.legend()\n        ax.set_title(f'{title} ({predictor_set.shape[0]} Samples)',fontweight='bold')\n    plot_scatter(axs[0], X_train, y_train, 'Training Set')\n    plot_scatter(axs[1], X_test, y_test, 'Testing Set')\n    axs[0].sharey(axs[1])\n    return plt.show()","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:25:46.728427Z","iopub.execute_input":"2025-03-31T07:25:46.728918Z","iopub.status.idle":"2025-03-31T07:25:46.750373Z","shell.execute_reply.started":"2025-03-31T07:25:46.728878Z","shell.execute_reply":"2025-03-31T07:25:46.749130Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"param_grid = {'max_depth': [None,30,32,35,37,38,39,40],'min_samples_split': [2,150,170,180,190,200]}\nbase_estimator = RandomForestClassifier(random_state=random_state_number)\nrfc_result = get_performance(param_grid, base_estimator, dataframes)\nrfc_result","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:25:46.751762Z","iopub.execute_input":"2025-03-31T07:25:46.752176Z","iopub.status.idle":"2025-03-31T07:43:35.157090Z","shell.execute_reply.started":"2025-03-31T07:25:46.752135Z","shell.execute_reply":"2025-03-31T07:43:35.155896Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_best_vector_clf(knn_result):\n\n    temp = knn_result[knn_result['Metric'] =='f1_macro']\n    temp2 = temp.iloc[temp['Best CV Metric Score'].idxmax()].to_frame().T\n    best_vector = temp2['Vector'].values[0]\n    best_clf = temp2['Calibrated Estimator'].values[0]\\\n    \n    return best_vector, best_clf\n\nbest_vector, best_clf =  get_best_vector_clf(rfc_result)\nvis_classification(vector_type = best_vector, estimator = best_clf)","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:43:35.158648Z","iopub.execute_input":"2025-03-31T07:43:35.159085Z","iopub.status.idle":"2025-03-31T07:43:36.521455Z","shell.execute_reply.started":"2025-03-31T07:43:35.159053Z","shell.execute_reply":"2025-03-31T07:43:36.520368Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DIMENSIONALITY REDUCTION","metadata":{}},{"cell_type":"code","source":"df_temp = rfc_result[rfc_result['Metric'] =='f1_macro']\n# df_temp['Calibrated Estimator']\nvector_rfc = df_temp[['Vector','Calibrated Estimator']].set_index('Vector').to_dict()['Calibrated Estimator']\nvector_rfc","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:43:36.522823Z","iopub.execute_input":"2025-03-31T07:43:36.523167Z","iopub.status.idle":"2025-03-31T07:43:36.534745Z","shell.execute_reply.started":"2025-03-31T07:43:36.523138Z","shell.execute_reply":"2025-03-31T07:43:36.533656Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"supported_columns_dict = {}\nfor df_name, df in dataframes.items():\n    X_train, X_test, y_train, y_test = train_test_split(dataframes[df_name], df_target, test_size=0.2, random_state=random_state_number)\n\n    selector = SelectFromModel(estimator=vector_rfc[df_name]).fit(X_train, y_train)\n    \n    filter_columns = selector.get_support()\n    dataframes[df_name] = dataframes[df_name][:, filter_columns]\n    \nshape_dim = [] ; df_names = []\nfor df_name, df in dataframes.items():\n    shape_dim.append(df.shape)\n    df_names.append(df_name)\nn_gram_df_dim = pd.DataFrame({'N-Gram Feature Vector':df_names, 'Data Dimension':shape_dim}) \nn_gram_df_dim","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:43:36.536205Z","iopub.execute_input":"2025-03-31T07:43:36.536534Z","iopub.status.idle":"2025-03-31T07:43:40.598625Z","shell.execute_reply.started":"2025-03-31T07:43:36.536506Z","shell.execute_reply":"2025-03-31T07:43:40.597480Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels = n_gram_df_dim['N-Gram Feature Vector'].values\nb4 = [shape[1] for shape in n_gram_df['Data Dimension'].values]\naf = [shape[1] for shape in n_gram_df_dim['Data Dimension'].values]\n\nx = np.arange(len(labels))  # the label locations\nwidth = 0.35  # the width of the bars\n\nfig, ax = plt.subplots(figsize=(10, 6))\nrects1 = ax.bar(x - width/2, b4, width, label='Before Dimensionality Reduction', color='skyblue')\nrects2 = ax.bar(x + width/2, af, width, label='After Dimensionality Reduction', color='lime')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylabel('Number Columns')\nax.set_title('Before and After Dimensionality Reduction')\nax.set_xticks(x, labels)\nax.set_xticklabels(ax.get_xticklabels(),rotation=30)\nax.legend()\n\nax.bar_label(rects1, padding=3)\nax.bar_label(rects2, padding=3)\n\nfig.tight_layout()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:43:40.599813Z","iopub.execute_input":"2025-03-31T07:43:40.600171Z","iopub.status.idle":"2025-03-31T07:43:40.862721Z","shell.execute_reply.started":"2025-03-31T07:43:40.600134Z","shell.execute_reply":"2025-03-31T07:43:40.861683Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"param_grid = {'n_neighbors': [5,7,9,11,13,15,17,19,21]}\nbase_estimator = KNeighborsClassifier()\nknn_result = get_performance(param_grid, base_estimator, dataframes)\nknn_result","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:43:40.864269Z","iopub.execute_input":"2025-03-31T07:43:40.864697Z","iopub.status.idle":"2025-03-31T07:43:48.931298Z","shell.execute_reply.started":"2025-03-31T07:43:40.864655Z","shell.execute_reply":"2025-03-31T07:43:48.930266Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_vector, best_clf =  get_best_vector_clf(knn_result)\nvis_classification(vector_type = best_vector, estimator = best_clf)","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:43:48.932475Z","iopub.execute_input":"2025-03-31T07:43:48.932751Z","iopub.status.idle":"2025-03-31T07:43:49.516628Z","shell.execute_reply.started":"2025-03-31T07:43:48.932726Z","shell.execute_reply":"2025-03-31T07:43:49.515528Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"param_grid = {'max_depth': [None,4,6,7,8,30,32,35],'min_samples_split': [2,3,4,5,35,10,16,20]}\nbase_estimator = DecisionTreeClassifier(random_state=random_state_number)\ndtc_result = get_performance(param_grid, base_estimator, dataframes)\ndtc_result","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:43:49.518188Z","iopub.execute_input":"2025-03-31T07:43:49.518600Z","iopub.status.idle":"2025-03-31T07:44:28.271181Z","shell.execute_reply.started":"2025-03-31T07:43:49.518556Z","shell.execute_reply":"2025-03-31T07:44:28.270202Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_vector, best_clf =  get_best_vector_clf(dtc_result)\nvis_classification(vector_type = best_vector, estimator = best_clf)","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:44:28.272598Z","iopub.execute_input":"2025-03-31T07:44:28.273028Z","iopub.status.idle":"2025-03-31T07:44:28.808948Z","shell.execute_reply.started":"2025-03-31T07:44:28.272985Z","shell.execute_reply":"2025-03-31T07:44:28.807887Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_result = pd.concat([knn_result, \n                      dtc_result,\n                      rfc_result\n                      ]\n                     ).reset_index(drop=True)\n\ndf_result.groupby(['Metric']).max()","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:44:28.810325Z","iopub.execute_input":"2025-03-31T07:44:28.810634Z","iopub.status.idle":"2025-03-31T07:44:28.830928Z","shell.execute_reply.started":"2025-03-31T07:44:28.810606Z","shell.execute_reply":"2025-03-31T07:44:28.829855Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# BEST CLASSIFIER AND FEATURE VECTOR","metadata":{}},{"cell_type":"code","source":"def get_best_result(df_result, metric_score):\n    df_result_t = df_result[df_result.Metric== 'precision_macro']\n    precision_macro_df = df_result_t.loc[df_result_t[metric_score].idxmax()].to_frame().T\n\n    df_result_t = df_result[df_result.Metric== 'recall_macro']\n    recall_macro_df = df_result_t.loc[df_result_t[metric_score].idxmax()].to_frame().T\n    \n    df_result_t = df_result[df_result.Metric== 'f1_macro']\n    f1_macro_df = df_result_t.loc[df_result_t[metric_score].idxmax()].to_frame().T\n\n    return pd.concat([precision_macro_df,recall_macro_df,f1_macro_df])","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:44:28.832431Z","iopub.execute_input":"2025-03-31T07:44:28.832731Z","iopub.status.idle":"2025-03-31T07:44:28.842501Z","shell.execute_reply.started":"2025-03-31T07:44:28.832705Z","shell.execute_reply":"2025-03-31T07:44:28.841588Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_cv_result = get_best_result(df_result, 'Best CV Metric Score')\ndisplay(best_cv_result)\ntemp = best_cv_result[best_cv_result['Metric'] == 'f1_macro']\nbest_clf = temp['Calibrated Estimator'].values[0]\nbest_vector = temp['Vector'].values[0]","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:44:28.843652Z","iopub.execute_input":"2025-03-31T07:44:28.844023Z","iopub.status.idle":"2025-03-31T07:44:28.876885Z","shell.execute_reply.started":"2025-03-31T07:44:28.843995Z","shell.execute_reply":"2025-03-31T07:44:28.875778Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"get_best_result(df_result, 'Test Predict Metric Score')","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:44:28.878516Z","iopub.execute_input":"2025-03-31T07:44:28.879375Z","iopub.status.idle":"2025-03-31T07:44:29.085065Z","shell.execute_reply.started":"2025-03-31T07:44:28.879327Z","shell.execute_reply":"2025-03-31T07:44:29.084018Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"EVALUATE ON CLASS LABELS","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(dataframes[best_vector], df_target, test_size=0.2, \\\n                                                    random_state=random_state_number)\nclf = best_clf.fit(X_train, y_train)\ny_test_pred= clf.predict(X_test)\ntarget_names = ['Discharge Summary', 'ENT', 'Neurosurgery']\nprint(classification_report(y_test,y_test_pred,target_names=target_names))","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:44:29.086616Z","iopub.execute_input":"2025-03-31T07:44:29.087060Z","iopub.status.idle":"2025-03-31T07:44:29.110432Z","shell.execute_reply.started":"2025-03-31T07:44:29.087020Z","shell.execute_reply":"2025-03-31T07:44:29.109273Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_predict = pd.DataFrame({'Actual Y Test': le.inverse_transform(y_test),'Best Prediction':le.inverse_transform(y_test_pred)})\nsample_predict.head(20)","metadata":{"execution":{"iopub.status.busy":"2025-03-31T07:44:29.111877Z","iopub.execute_input":"2025-03-31T07:44:29.112326Z","iopub.status.idle":"2025-03-31T07:44:29.125667Z","shell.execute_reply.started":"2025-03-31T07:44:29.112286Z","shell.execute_reply":"2025-03-31T07:44:29.124521Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}}]}